{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c0a5ff-0a3c-4b9f-a6d1-167c0dedb876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from collections import deque\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "from gymnasium.envs.registration import register\n",
    "from stable_baselines3 import PPO, A2C\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\n",
    "\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch as th\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from environment.mangoEnv import MangoEnv, MAX_AMOUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94ebdb9-6d93-46ee-a3aa-e8cdba65abde",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join(os.getcwd(), 'Logs')\n",
    "opt_path = os.path.join(os.getcwd(), 'Opt')\n",
    "log_path_for_opt = os.path.join(os.getcwd(), 'Opt', 'Logs')\n",
    "checkpoints_path = os.path.join(os.getcwd(), 'Train')\n",
    "#!tensorboard --logdir=Training\\Logs - to watch log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6352249e-7f58-4019-b7d2-f264074e4c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episode_steps = 25000\n",
    "max_episode_steps_eval = 100\n",
    "fine_tune_timesteps = 2000000\n",
    "callback_freq = 100000\n",
    "training_alg = 'PPO' # 'PPO', 'A2C', 'REC_PPO'\n",
    "\n",
    "# Environment parameters\n",
    "env_args = {\n",
    "    'render_mode': None,\n",
    "    'max_episode_steps': max_episode_steps,\n",
    "    'ext_mngo_price_mean': 0.1,\n",
    "    'init_mngo_pool_balance': 1e6 / MAX_AMOUNT,\n",
    "    'init_usdc_pool_balance': 1e5 / MAX_AMOUNT,\n",
    "    'init_treasury_size_usdc': 100e6 / MAX_AMOUNT,\n",
    "    'mngo_collateral_factor': 1.5,\n",
    "    'arb_efficiency_factor': 0.5,\n",
    "}\n",
    "\n",
    "# Optimization parameters\n",
    "N_TRIALS = 100  # Maximum number of trials\n",
    "N_JOBS = 1 # Number of jobs to run in parallel\n",
    "N_STARTUP_TRIALS = 5  # Stop random sampling after N_STARTUP_TRIALS\n",
    "N_EVALUATIONS = 5  # Number of evaluations during the training\n",
    "N_TIMESTEPS = int(1e5)  # Training budget\n",
    "EVAL_FREQ = int(N_TIMESTEPS / N_EVALUATIONS)\n",
    "N_EVAL_EPISODES = 10\n",
    "TIMEOUT = int(60 * 60 * 3)\n",
    "\n",
    "if training_alg in ['A2C', 'PPO']:\n",
    "    DEFAULT_HYPERPARAMS = {\n",
    "        \"policy\": \"MlpPolicy\",\n",
    "    }\n",
    "elif training_alg in ['REC_PPO']:\n",
    "    DEFAULT_HYPERPARAMS = {\n",
    "        \"policy\": \"MlpLstmPolicy\",\n",
    "    }    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2b0711-15ec-42fd-af93-a7525d801711",
   "metadata": {},
   "outputs": [],
   "source": [
    "register(\n",
    "     id=\"MangoEnv-v0\",\n",
    "     entry_point=\"environment.mangoEnv:MangoEnv\",\n",
    "     max_episode_steps=100,\n",
    ")\n",
    "\n",
    "env = gym.make(\"MangoEnv-v0\", **env_args)\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb57dcc4-49fe-43d5-b360-57dcdcd179bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MangoEnv-v0\", **env_args)\n",
    "success = 0\n",
    "\n",
    "for jj in range(N_EVAL_EPISODES):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        if reward == 10.0:\n",
    "            success += 1\n",
    "        done = terminated or truncated\n",
    "print(f\"{success} / {N_EVAL_EPISODES} successes detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9025f0-dc44-4a0c-847c-238c8b2151d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = gym.make(\"MangoEnv-v0\", **env_args)\n",
    "eval_env = Monitor(eval_env, log_path)\n",
    "eval_env = DummyVecEnv([lambda: eval_env])\n",
    "if training_alg == 'PPO':\n",
    "    model = PPO('MlpPolicy', env, verbose=0, tensorboard_log=log_path)\n",
    "elif training_alg == 'REC_PPO':\n",
    "    model = RecurrentPPO('MlpLstmPolicy', env, verbose=0, tensorboard_log=log_path)\n",
    "else:\n",
    "    model = A2C('MlpPolicy', env, verbose=0, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5f6f62-bc04-44e1-af2e-bb70d249b91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=False, render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1865a770-8d12-4ebb-8729-6ff1a5580613",
   "metadata": {},
   "source": [
    "## Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda91df0-1ff2-4be0-b9d5-414ec8705f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_a2c_params(trial: optuna.Trial):\n",
    "    \"\"\"\n",
    "    Sampler for A2C hyperparameters.\n",
    "\n",
    "    :param trial: Optuna trial object\n",
    "    :return: The sampled hyperparameters for the given trial.\n",
    "    \"\"\"\n",
    "    gamma = 1.0 - trial.suggest_float(\"gamma\", 0.0001, 0.1, log=True)\n",
    "    max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.3, 5.0, log=True)\n",
    "    n_steps = 2 ** trial.suggest_int(\"exponent_n_steps\", 3, 10)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1, log=True)\n",
    "    net_arch = trial.suggest_categorical(\"net_arch\", [\"tiny\", \"small\", \"medium\"]) # [\"tiny\", \"small\", \"medium\"]\n",
    "    activation_fn = trial.suggest_categorical(\"fn\", [\"tanh\", \"relu\"])\n",
    "\n",
    "    # Display true values\n",
    "    trial.set_user_attr(\"gamma_\", gamma)\n",
    "    trial.set_user_attr(\"n_steps\", n_steps)\n",
    "\n",
    "    if net_arch == \"tiny\":\n",
    "        net_arch = {\"pi\": [64], \"vf\":[64]}\n",
    "        trial.set_user_attr(\"net_arch\", {\"pi\": [64], \"vf\":[64]})\n",
    "    elif net_arch == \"small\":\n",
    "        net_arch = {\"pi\": [64, 64], \"vf\": [64, 64]}\n",
    "        trial.set_user_attr(\"net_arch\", {\"pi\": [64, 64], \"vf\": [64, 64]})\n",
    "    elif net_arch == \"medium\":\n",
    "        net_arch = {\"pi\": [64, 64, 128], \"vf\": [64, 64, 128]}\n",
    "        trial.set_user_attr(\"net_arch\", {\"pi\": [64, 64, 128], \"vf\": [64, 64, 128]})\n",
    "\n",
    "    trial.set_user_attr(\"activation_fn\", {\"tanh\": nn.Tanh, \"relu\": nn.ReLU}[activation_fn])\n",
    "    activation_fn = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU}[activation_fn]\n",
    "\n",
    "    return {\n",
    "        \"n_steps\": n_steps,\n",
    "        \"gamma\": gamma,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"max_grad_norm\": max_grad_norm,\n",
    "        \"policy_kwargs\": {\n",
    "            \"net_arch\": net_arch,\n",
    "            \"activation_fn\": activation_fn,\n",
    "        },\n",
    "    }\n",
    "\n",
    "def sample_ppo_params(trial: optuna.Trial, batch_size=64):\n",
    "    \"\"\"\n",
    "    Sampler for PPO hyperparameters.\n",
    "\n",
    "    :param trial: Optuna trial object\n",
    "    :return: The sampled hyperparameters for the given trial.\n",
    "    \"\"\"\n",
    "    n_steps = trial.suggest_int('n_steps', 32*batch_size, 256*batch_size, step=batch_size)\n",
    "    gamma = 1.0 - trial.suggest_float(\"gamma\", 0.0001, 0.1, log=True)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-6, 1, log=True)\n",
    "    clip_range = trial.suggest_float('clip_range', 0.1, 0.4)\n",
    "    ent_coef = trial.suggest_float('ent_coef', 0.0, 1.0)\n",
    "    gae_lambda = trial.suggest_float('gae_lambda', 0.8, 0.99)\n",
    "    net_arch = trial.suggest_categorical(\"net_arch\", [\"tiny\", \"small\", \"medium\"])\n",
    "    activation_fn = trial.suggest_categorical(\"fn\", [\"tanh\", \"relu\"])\n",
    "\n",
    "    # Display true values\n",
    "    trial.set_user_attr(\"gamma_\", gamma)\n",
    "\n",
    "    if net_arch == \"tiny\":\n",
    "        net_arch = {\"pi\": [64], \"vf\":[64]}\n",
    "        trial.set_user_attr(\"net_arch\", {\"pi\": [64], \"vf\":[64]})\n",
    "    elif net_arch == \"small\":\n",
    "        net_arch = {\"pi\": [64, 64], \"vf\": [64, 64]}\n",
    "        trial.set_user_attr(\"net_arch\", {\"pi\": [64, 64], \"vf\": [64, 64]})\n",
    "    elif net_arch == \"medium\":\n",
    "        net_arch = {\"pi\": [64, 64, 128], \"vf\": [64, 64, 128]}\n",
    "        trial.set_user_attr(\"net_arch\", {\"pi\": [64, 64, 128], \"vf\": [64, 64, 128]})\n",
    "\n",
    "    trial.set_user_attr(\"activation_fn\", {\"tanh\": nn.Tanh, \"relu\": nn.ReLU}[activation_fn])\n",
    "    activation_fn = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU}[activation_fn]\n",
    "\n",
    "    return {\n",
    "        \"n_steps\": n_steps,\n",
    "        \"gamma\": gamma,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"clip_range\": clip_range,\n",
    "        \"gae_lambda\": gae_lambda,\n",
    "        \"ent_coef\": ent_coef,\n",
    "        \"policy_kwargs\": {\n",
    "            \"net_arch\": net_arch,\n",
    "            \"activation_fn\": activation_fn,\n",
    "        },\n",
    "    }\n",
    "\n",
    "def sample_reccurent_ppo_params(trial: optuna.Trial):\n",
    "    \"\"\"\n",
    "    Sampler for recurrent PPO hyperparameters.\n",
    "\n",
    "    :param trial: Optuna trial object\n",
    "    :return: The sampled hyperparameters for the given trial.\n",
    "    \"\"\"\n",
    "    batch_size = trial.suggest_categorical('batch_size', [2**x for x in range(5, 11)])\n",
    "    n_steps = trial.suggest_int('n_steps', batch_size, 64*batch_size, step=batch_size)\n",
    "    gamma = 1.0 - trial.suggest_float(\"gamma\", 0.0001, 0.1, log=True)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-7, 1, log=True)\n",
    "    clip_range = trial.suggest_float('clip_range', 0.1, 0.4)\n",
    "    ent_coef = trial.suggest_float('ent_coef', 0.0, 1.0)\n",
    "    gae_lambda = trial.suggest_float('gae_lambda', 0.8, 0.99)\n",
    "\n",
    "    # Display true values\n",
    "    trial.set_user_attr(\"gamma_\", gamma)\n",
    "\n",
    "    return {\n",
    "        \"batch_size\": batch_size,\n",
    "        \"n_steps\": n_steps,\n",
    "        \"gamma\": gamma,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"clip_range\": clip_range,\n",
    "        \"gae_lambda\": gae_lambda,\n",
    "        \"ent_coef\": ent_coef,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548d8405-d4a8-4dd0-ba3b-8986443adcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrialEvalCallback(EvalCallback):\n",
    "    \"\"\"\n",
    "    Callback used for evaluating and reporting a trial.\n",
    "    \n",
    "    :param eval_env: Evaluation environement\n",
    "    :param trial: Optuna trial object\n",
    "    :param n_eval_episodes: Number of evaluation episodes\n",
    "    :param eval_freq:   Evaluate the agent every ``eval_freq`` call of the callback.\n",
    "    :param deterministic: Whether the evaluation should\n",
    "        use a stochastic or deterministic policy.\n",
    "    :param verbose:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_env: gym.Env,\n",
    "        trial: optuna.Trial,\n",
    "        n_eval_episodes: int = 5,\n",
    "        eval_freq: int = 10000,\n",
    "        deterministic: bool = True,\n",
    "        verbose: int = 0,\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            eval_env=eval_env,\n",
    "            n_eval_episodes=n_eval_episodes,\n",
    "            eval_freq=eval_freq,\n",
    "            deterministic=deterministic,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.trial = trial\n",
    "        self.eval_idx = 0\n",
    "        self.is_pruned = False\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            # Evaluate policy (done in the parent class)\n",
    "            super()._on_step()\n",
    "            self.eval_idx += 1\n",
    "            # Send report to Optuna\n",
    "            self.trial.report(self.last_mean_reward, self.eval_idx)\n",
    "            # Prune trial if need\n",
    "            if self.trial.should_prune():\n",
    "                self.is_pruned = True\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0be66d-4bb4-4da7-8175-c67766853c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial, alg=training_alg) -> float:\n",
    "    \"\"\"\n",
    "    Objective function using by Optuna to evaluate\n",
    "    one configuration (i.e., one set of hyperparameters).\n",
    "\n",
    "    Given a trial object, it will sample hyperparameters,\n",
    "    evaluate it and report the result (mean episodic reward after training)\n",
    "\n",
    "    :param trial: Optuna trial object\n",
    "    :return: Mean episodic reward after training\n",
    "    \"\"\"\n",
    "\n",
    "    kwargs = DEFAULT_HYPERPARAMS.copy()\n",
    "    env = gym.make(\"MangoEnv-v0\", **env_args)\n",
    "    env = Monitor(env, log_path)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecFrameStack(env, 5, channels_order='last')\n",
    "    kwargs['env'] = env\n",
    "    if alg == 'PPO':\n",
    "        other_params = sample_ppo_params(trial)\n",
    "        kwargs.update(other_params)\n",
    "        model = PPO(**kwargs)\n",
    "    elif alg == 'REC_PPO':\n",
    "        other_params = sample_reccurent_ppo_params(trial)\n",
    "        kwargs.update(other_params)\n",
    "        model = RecurrentPPO(**kwargs)\n",
    "    else:\n",
    "        other_params = sample_a2c_params(trial)\n",
    "        kwargs.update(other_params)\n",
    "        model = A2C(**kwargs)\n",
    "    eval_env = gym.make(\"MangoEnv-v0\", **env_args)\n",
    "    eval_env = Monitor(eval_env, log_path)\n",
    "    eval_env = DummyVecEnv([lambda: eval_env])\n",
    "    eval_env = VecFrameStack(eval_env, 5, channels_order='last')\n",
    "    eval_callback = TrialEvalCallback(eval_env, trial, N_EVAL_EPISODES, EVAL_FREQ, deterministic=False, verbose=0)\n",
    "\n",
    "    nan_encountered = False\n",
    "    try:\n",
    "        # Train the model\n",
    "        model.learn(N_TIMESTEPS, callback=eval_callback)\n",
    "        model.save(os.path.join(opt_path, 'trial_{}_best_model'.format(trial.number)))\n",
    "    except AssertionError as e:\n",
    "        # Sometimes, random hyperparams can generate NaN\n",
    "        print(e)\n",
    "        nan_encountered = True\n",
    "    finally:\n",
    "        # Free memory\n",
    "        model.env.close()\n",
    "        eval_envs.close()\n",
    "\n",
    "    # Tell the optimizer that the trial failed\n",
    "    if nan_encountered:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    if eval_callback.is_pruned:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return eval_callback.last_mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21afa614-818e-4a07-ac73-34dca0ccd815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pytorch num threads to 1 for faster training\n",
    "th.set_num_threads(1)\n",
    "# Select the sampler, can be random, TPESampler, CMAES, ...\n",
    "sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
    "# Do not prune before 1/3 of the max budget is used\n",
    "pruner = MedianPruner(\n",
    "    n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=N_EVALUATIONS // 3\n",
    ")\n",
    "# Create the study and start the hyperparameter optimization\n",
    "study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n",
    "\n",
    "try:\n",
    "    study.optimize(objective, n_trials=N_TRIALS, n_jobs=N_JOBS, timeout=TIMEOUT)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(f\"  Value: {trial.value}\")\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "print(\"  User attrs:\")\n",
    "for key, value in trial.user_attrs.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "model_params = trial.params.copy()\n",
    "model_params['gamma'] = trial.user_attrs['gamma_']\n",
    "\n",
    "if training_alg in ['PPO', 'A2C']:\n",
    "    model_params.pop('net_arch', None)\n",
    "    model_params.pop('fn', None)\n",
    "    model_params['policy_kwargs'] = {\n",
    "        'net_arch': trial.user_attrs['net_arch'],\n",
    "        'activation_fn': trial.user_attrs['activation_fn'],\n",
    "    }\n",
    "if training_alg == 'A2C':\n",
    "    model_params.pop('exponent_n_steps', None)\n",
    "    model_params['n_steps'] = trial.user_attrs['n_steps']\n",
    "\n",
    "fig1 = plot_optimization_history(study)\n",
    "# fig2 = plot_param_importances(study) # function doesn't work on windows!\n",
    "\n",
    "fig1.show()\n",
    "# fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a46f44-8120-4a24-ad0c-76b6496d9e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba52597-ee0d-4281-a570-f12a02ce1008",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c24f53d-ed60-4190-ac5e-2761908549a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffef2d7-3740-45a7-8f7b-7784388e7d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=callback_freq, save_path=checkpoints_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba545519-dba7-4914-9328-ebfaae9bb6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MangoEnv-v0\", **env_args)\n",
    "env = Monitor(env, log_path)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 5, channels_order='last')\n",
    "\n",
    "if training_alg == 'PPO':\n",
    "    model = PPO('MlpPolicy', env, verbose=0, tensorboard_log=log_path, **model_params)\n",
    "elif training_alg == 'REC_PPO':\n",
    "    model = RecurrentPPO('MlpLstmPolicy', env, verbose=0, tensorboard_log=log_path, **model_params)\n",
    "else:\n",
    "    model = A2C('MlpPolicy', env, verbose=0, tensorboard_log=log_path, **model_params)\n",
    "\n",
    "print(f\"Coefficients from trial {trial.number} loaded.\")\n",
    "model.load(os.path.join(opt_path, f\"trial_{trial.number}_best_model.zip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a6f553-f0e6-4650-a19b-2823c4f9df90",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = gym.make(\"MangoEnv-v0\", **env_args)\n",
    "eval_env = Monitor(eval_env, log_path)\n",
    "eval_env = DummyVecEnv([lambda: eval_env])\n",
    "eval_env = VecFrameStack(eval_env, 5, channels_order='last')\n",
    "evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=False, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ec5340-4752-4a58-aff8-b6f4c0c8529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MangoEnv-v0\", **env_args)\n",
    "env = Monitor(env, log_path)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 5, channels_order='last')\n",
    "success = 0\n",
    "\n",
    "for jj in range(N_EVAL_EPISODES):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=False)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        if reward == 10.0:\n",
    "            success += 1\n",
    "print(f\"{success} / {N_EVAL_EPISODES} successes detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f54d6-6d91-4eae-82e5-6123c2370251",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=fine_tune_timesteps, progress_bar=True, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e151992e-399c-4c59-b8ec-9aa1742c8d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = gym.make(\"MangoEnv-v0\", **env_args)\n",
    "eval_env = Monitor(eval_env, log_path)\n",
    "eval_env = DummyVecEnv([lambda: eval_env])\n",
    "eval_env = VecFrameStack(eval_env, 5, channels_order='last')\n",
    "evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=False, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5117bb8-9fdc-483b-a9a4-6fd6c24f0045",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MangoEnv-v0\", **env_args)\n",
    "env = Monitor(env, log_path)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 5, channels_order='last')\n",
    "obs = env.reset()\n",
    "total_reward = 0\n",
    "done = False\n",
    "d = deque(maxlen=2)\n",
    "attack = 0\n",
    "\n",
    "for ii in range(max_episode_steps_eval):\n",
    "    action, _ = model.predict(obs, deterministic=False)\n",
    "    print(f\"action {ii}: {env.venv.env_method('convert_action_rl_to_human', action.squeeze())[0]}, mngo_spot_price: {env.venv.get_attr('amm')[0].get_price()}\")\n",
    "    d.appendleft(action.squeeze()[0])\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if d == deque([2, 0]) and reward > 0:\n",
    "        print(f\"Attack on treasury!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        attack += 1\n",
    "    if reward > 0:\n",
    "        print(f\"reward at {ii} step: {reward.squeeze()}!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    else:\n",
    "        print(f\"reward at {ii} step: {reward.squeeze()}\")\n",
    "    total_reward += reward\n",
    "    if done.squeeze():\n",
    "        break\n",
    "    \n",
    "print(f\"Done: {done.squeeze()}, total reward: {total_reward.squeeze()}, attacks: {attack}\")\n",
    "print(f\"Treasury: {env.venv.get_attr('mango')[0].treasury_usdc}\")\n",
    "print(f\"Health_factor: {env.venv.get_attr('mango')[0].get_user_health_factor()}, debt is bad: {env.venv.get_attr('mango')[0].debt_is_bad()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2c383f-d01d-466d-b722-56d2ba5421fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MangoEnv-v0\", **env_args)\n",
    "env = Monitor(env, log_path)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 5, channels_order='last')\n",
    "success = 0\n",
    "\n",
    "for jj in range(N_EVAL_EPISODES):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=False)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        if reward == 10.0:\n",
    "            success += 1\n",
    "print(f\"{success} / {N_EVAL_EPISODES} successes detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5267b63-75ac-40b1-9fc9-9cfe7224f333",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
